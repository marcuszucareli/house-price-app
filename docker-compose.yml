services:
  api:
    build:
      context: .
      dockerfile: ./api/Dockerfile
    depends_on:
      - mlflow_client
    env_file:
      - /mnt/blockvolume/home-estimate-ai/envs/api/.env
    volumes:
      - /mnt/blockvolume/home-estimate-ai:/service/volumes
    ports:
      - "8000:8000"
    networks:
      - app-network
    command: uvicorn api.api:app --host 0.0.0.0 --port 8000 --reload

  app:
    build:
      context: .
      dockerfile: ./app/Dockerfile
      args:
          APP_ENV: prod
    depends_on:
      - mlflow_client
    env_file:
      - /mnt/blockvolume/home-estimate-ai/envs/app/.env
    ports:
      - "8080:8080"
    networks:
      - app-network
    command: streamlit run ./app/main.py --server.port=8080 --server.address=0.0.0.0 --server.headless=true

  mlflow_client:
    build:
      context: .
      dockerfile: ./mlflow_client/Dockerfile
    env_file:
      - /mnt/blockvolume/home-estimate-ai/envs/mlflow_client/.env
    volumes:
      - /mnt/blockvolume/home-estimate-ai:/service/volumes
    ports:
      - "5000:5000"
    networks:
      - app-network
    command: sh -c "sleep infinity"

networks:
  app-network:
